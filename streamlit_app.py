import streamlit as st
import os
from dotenv import load_dotenv

# We assume retriever.py is in a 'core' directory
# Make sure you have an __init__.py file in the 'core' directory
from core.retriever import InformationRetriever

# --- Page Configuration ---
st.set_page_config(
    page_title="KSU Engineering Q&A",
    page_icon="ğŸ¤–",
    layout="centered"
)

# --- Environment Variables & Secrets ---
# For local development, create a .env file with your OPENAI_API_KEY
load_dotenv()
# For Streamlit Cloud, set the secret in the app settings
openai_api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
if not openai_api_key:
    st.error("Ù…ÙØªØ§Ø­ OpenAI API ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¶Ø§ÙØªÙ‡ Ù„Ù…Ù„Ù .env Ø£Ùˆ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©.")
    st.stop()


# --- Load Retriever (Cached for Performance) ---
@st.cache_resource
def load_retriever_system():
    """
    Loads the InformationRetriever instance using Streamlit's caching.
    The retriever will be loaded only once.
    """
    print("Initializing InformationRetriever for the first time...")
    # --- Configuration ---
    # Make sure these paths are correct within your project structure
    excel_file_path = os.path.join("data", "crawled_chunks_final.xlsx")
    embeddings_cache_path = os.path.join("data", "ksa_engineering_embeddings.npy")
    openai_model = "gpt-4o" # Recommended model for quality
    # Use your fine-tuned model from Hugging Face
    embedding_model_name = "TheMohanad1/Fine-Tuned-E5"

    # Ensure data file exists before initializing
    if not os.path.exists(excel_file_path):
        st.error(f"Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ '{excel_file_path}'.")
        # Stop the app if data files are missing
        st.stop()

    retriever_instance = InformationRetriever(
        openai_model=openai_model,
        excel_path=excel_file_path,
        embeddings_path=embeddings_cache_path,
        embedding_model_name=embedding_model_name
    )
    print("InformationRetriever loaded successfully.")
    return retriever_instance

try:
    retriever = load_retriever_system()
except Exception as e:
    st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…: {e}")
    st.stop()


# --- Chat Interface ---
st.title("ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯ ÙƒÙ„ÙŠØ© Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø¨Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø³Ø¹ÙˆØ¯")
st.caption("Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ Ø­ÙˆÙ„ Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©ØŒ Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø¨ÙˆÙ„ØŒ Ø£Ùˆ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø© ØªØ®Øµ Ø§Ù„ÙƒÙ„ÙŠØ©.")

# Initialize chat history in session state
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display previous chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Handle user input
if prompt := st.chat_input("Ù…Ø§ Ù‡Ùˆ Ø³Ø¤Ø§Ù„ÙƒØŸ"):
    # Add user message to history and display it
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generate and display assistant's response
    with st.chat_message("assistant"):
        with st.spinner("...Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙØ¶Ù„ Ø¥Ø¬Ø§Ø¨Ø©"):
            # Prepare chat history for the retriever
            history_for_retriever = [
                msg for msg in st.session_state.messages if msg["role"] in ['user', 'assistant']
            ]
            
            # Get the result from your powerful backend
            result = retriever.search_and_answer(
                query=prompt,
                history=history_for_retriever,
                top_k=7
            )
            
            response_text = result.get("answer", "Ø¹ÙÙˆØ§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø¥ÙŠØ¬Ø§Ø¯ Ø¥Ø¬Ø§Ø¨Ø©.")
            sources = result.get("sources", [])

            st.markdown(response_text)
            
            # Display sources in an expander
            if sources:
                with st.expander("ğŸ“š Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©"):
                    for i, source in enumerate(sources):
                        st.write(f"**Ø§Ù„Ù…ØµØ¯Ø± Ø±Ù‚Ù… {i+1}:**")
                        st.info(source.get('text', 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ Ù„Ù„Ù…ØµØ¯Ø±.'))
                        st.write(f"[Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±]({source.get('source_url', '#')})")
                        st.markdown("---")


    # Add assistant response to chat history
    st.session_state.messages.append({"role": "assistant", "content": response_text})


